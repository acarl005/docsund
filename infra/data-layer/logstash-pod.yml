apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-configmap
data:
  csv_to_json.py: |
    import sys, csv, json
    csv.field_size_limit(sys.maxsize)
    with open("/data/email_data.csv") as csv_f:
        csv_reader = csv.DictReader(csv_f)
        with open("/data/email_data.jsonl", "w") as json_f:
            for record in csv_reader:
                json_f.write(json.dumps(record))
                json_f.write("\n")
  logstash.conf: |
    input {
      file {
        path => ["/data/email_data.jsonl"]
        start_position => "beginning"
        codec => json
      }
    }
    output {
      elasticsearch {
        hosts => "http://${ELASTICSEARCH_SERVICE_HOST}:${ELASTICSEARCH_SERVICE_PORT_MAIN}"
        index => "emails"
      }
    }
---
apiVersion: v1
kind: Pod
metadata:
  name: docsund-logstash
  labels:
    app: docsund-logstash
spec:
  containers:
    - name: docsund-logstash
      image: logstash:7.2.0
      volumeMounts:
        - name: email-data
          mountPath: /data
        - name: config-files
          mountPath: /usr/share/logstash/config/logstash.conf
          subPath: logstash.conf
      resources:
        requests:
          ephemeral-storage: 4Gi
      ports:
        - containerPort: 9600
          name: main
  initContainers:
    - name: download-csv
      image: garland/aws-cli-docker
      volumeMounts:
        - name: email-data
          mountPath: /data
        - name: config-files
          mountPath: /config/csv_to_json.py
          subPath: csv_to_json.py
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-secrets
              key: access_key_id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-secrets
              key: secret_access_key
        - name: AWS_S3_CSV_PATH
          valueFrom:
            secretKeyRef:
              name: aws-secrets
              key: s3_csv_path
      command:
        - sh
        - "-c"
        - "aws s3 cp ${AWS_S3_CSV_PATH:-NO_S3_PATH_PROVIDED} /data/email_data.csv && python /config/csv_to_json.py"
    - name: wait-for-elasticsearch
      image: darthcabs/tiny-tools:1
      args:
        - /bin/bash
        - -c
        - >
          set -x;
          while [[ "$(curl -s -o /dev/null -w ''%{http_code}'' http://elasticsearch:9200/_cat/health)" != "200" ]]; do 
            echo 'waiting for elasticsearch'
            sleep 10;
          done
  volumes:
    - name: email-data
      hostPath:
        path: /csv-path
    - name: config-files
      configMap:
        name: logstash-configmap
  nodeSelector:
    group: workers
